[
  {
    "objectID": "posts/quicksummaries_localspindles/index.html",
    "href": "posts/quicksummaries_localspindles/index.html",
    "title": "Quick paper summary: Increased sleep spindles in regions engaged during motor learning predict memory consolidation (2025)",
    "section": "",
    "text": "This is a non-technical (or at least less technical) summary of the paper “Increased sleep spindles in regions engaged during motor learning predict memory consolidation” by Sjøgård et al. (2025), which was recently published in Journal of Neuscience."
  },
  {
    "objectID": "posts/quicksummaries_localspindles/index.html#background-learning-and-consolidation",
    "href": "posts/quicksummaries_localspindles/index.html#background-learning-and-consolidation",
    "title": "Quick paper summary: Increased sleep spindles in regions engaged during motor learning predict memory consolidation (2025)",
    "section": "Background: Learning and consolidation",
    "text": "Background: Learning and consolidation"
  },
  {
    "objectID": "posts/fwer-correction-part7/index.html",
    "href": "posts/fwer-correction-part7/index.html",
    "title": "Adjusted P-Values and Inference with Permutation-Based Correction",
    "section": "",
    "text": "In this post, I delve into adjusted p-values in the context of permutation-based family-wise error rate (FWER) correction. I will define adjusted p-values formally, derive them from the permutation max-statistic distribution, and explain why using adjusted p-values controls the FWER. I also provide R code to compute adjusted p-values on synthetic data, and discuss when to use adjusted p-values versus a threshold-based correction approach. Throughout, I try to maintain a rigorous but accessible tone, with intuitive explanations for my specific collaborators/trainees and examples from neuroscience (e.g. MEG/EEG parcel-level analysis) and other fields.\nFormal Definition of Adjusted P-Values When multiple hypothesis tests are performed together (forming a family of tests), an adjusted p-value for a given hypothesis is the p-value recalibrated to account for the multiple testing. Formally, the adjusted p-value for hypothesis \\(H_{0,i}\\) is defined as the smallest significance level \\(\\alpha\\) at which \\(H_{0,i}\\) would be rejected, given that the entire family of tests is considered. In other words, \\(p_{i,\\text{adj}} = \\inf{\\alpha: H_{0,i}\\) is rejected by the multiple-testing procedure at level \\(\\alpha\\). If we reject all hypotheses with \\(p_{i,\\text{adj}} &lt; \\alpha\\), then by construction the procedure controls the family-wise error rate at (or below) \\(\\alpha\\).\nAdjusted p-value via max-statistic: For each hypothesis \\(i\\), the permutation adjusted p-value \\(p_{i,\\text{adj}}\\) can be obtained by comparing the observed test statistic \\(T_i^{obs}\\) to the max-statistic null distribution. Specifically, if larger \\(T\\) indicates greater significance (right-tailed test), the adjusted p-value is the probability of seeing a max statistic as extreme as \\(T_i^{obs}\\) under the null. Formally:\n\\[\np_{i,\\text{adj}} = \\Pr\\left(\\max_{1\\leq j\\leq m} T_j \\geq T_i^{\\text{obs}}\\right)\n\\]\nThe numerator counts."
  },
  {
    "objectID": "posts/kmeans-clusterdetermination-1/index.html",
    "href": "posts/kmeans-clusterdetermination-1/index.html",
    "title": "Part 1: Foundations of Clustering and K-Means",
    "section": "",
    "text": "I have been working a lot with k-means clustering, especially finding a good way to set the number of clusters (i.e., k). Some of the approaches are outright not useable for my specific use cases, while others are good in certain areas and bad in others. I’ve been using this blog space to formalize some thoughts while trying to develop alternative approaches to setting the appropriate k. Some background is provided first, to properly set the stage for the coming suggested changes to the current best models in use in the literature."
  },
  {
    "objectID": "posts/kmeans-clusterdetermination-1/index.html#introduction-to-clustering",
    "href": "posts/kmeans-clusterdetermination-1/index.html#introduction-to-clustering",
    "title": "Part 1: Foundations of Clustering and K-Means",
    "section": "Introduction to Clustering",
    "text": "Introduction to Clustering\nClustering is the task of grouping a set of objects such that those in the same group (cluster) are more similar to each other than to those in other groups. In essence, a good clustering yields high intra-cluster similarity and low inter-cluster similarity. This is a form of unsupervised learning, meaning the data come without predefined category labels – the algorithm must discover any natural groupings. Humans intuitively cluster things all the time (e.g. grouping books by genre or animals by species), and clustering algorithms aim to mimic this by detecting structure in data without explicit guidance.\nClustering has a rich history across disciplines. The concept originated in anthropology in the early 1930s and was introduced to psychology by Joseph Zubin (1938) and Robert Tryon (1939). Over decades, many clustering methods have been developed, from hierarchical algorithms that build tree-like partitions to partitional algorithms that directly partition data into a set number of clusters. The very notion of a “cluster” is somewhat philosophical: there isn’t a single rigorous definition, which is why dozens of clustering algorithms exist. Each algorithm embodies a different idea of what constitutes a cluster (compactness, density, connectedness, etc.) and is suited to different data shapes and contexts.\nIn practice, clustering is a fundamental tool in exploratory data analysis, used in fields as diverse as biology (e.g. grouping cells by gene expression), marketing (segmenting customers), image analysis, and neuroscience. For example, neuroscientists might use clustering to group neurons with similar activity patterns, or to identify distinct patient subtypes from brain scan data. Philosophically, clustering touches on the problem of category formation – how to draw boundaries in a continuum of observations. The brain itself performs a form of clustering when it categorizes sensory inputs into perceptions (think of how we segment colors into discrete categories or group similar sounds into phonemes). Thus, beyond its mathematical formulation, clustering resonates with how we naturally seek patterns and order in the world."
  },
  {
    "objectID": "posts/kmeans-clusterdetermination-1/index.html#the-k-means-clustering-algorithm",
    "href": "posts/kmeans-clusterdetermination-1/index.html#the-k-means-clustering-algorithm",
    "title": "Part 1: Foundations of Clustering and K-Means",
    "section": "The K-Means Clustering Algorithm",
    "text": "The K-Means Clustering Algorithm\nOne of the most popular and straightforward partitional clustering methods is K-means clustering. K-means is a centroid-based clustering algorithm: it represents each cluster by the mean (centroid) of its members, and tries to find the optimal positions of these centroids.\n\nIntuition and Objective\nIntuitively, K-means seeks to partition the data into **K clusters* (where K is a chosen constant) such that each data point belongs to the cluster with the nearest centroid. It’s like placing K points (“centers”) in the space and assigning each observation to the closest center; then moving the centers to the average of the points assigned to them, and repeating until things stabilize. The objective is to minimize the total within-cluster variance, often expressed as the within-cluster sum of squares (WCSS) or “inertia.” Formally, if we denote clusters by \\(S_1, S_2, ..., S_K\\) and \\(\\mu_i\\) as the mean (centroid) of cluster \\(i\\), K-means aims to solve:\n\\[\n\\arg\\min_{S_1, \\ldots, S_K} \\sum_{i=1}^{K} \\sum_{\\mathbf{x} \\in S_i} \\left\\| \\mathbf{x} - \\boldsymbol{\\mu}_i \\right\\|^2\n\\]\ni.e. find the partition \\(S = {S_1,\\dots,S_K}\\) that minimizes the sum of squared Euclidean distances of points from their cluster centroids. This objective is a non-convex optimization problem, known to be NP-hard in general for an arbitrary number of clusters K. However, the beauty of K-means lies in a simple iterative heuristic (often called Lloyd’s algorithm or the K-means algorithm) that usually converges quickly to a good solution in practice.\n\n\nThe K-Means Algorithm Steps\nThe K-means algorithm works as follows:\n\n\nInitialization: Choose K initial centroids. These can be selected randomly from the data points or via smarter schemes (like K-means++ which picks initial centers probabilistically to spread them out).\n\n\nAssignment step: Assign each data point to the nearest centroid (using Euclidean distance). This forms K clusters based on current centroid locations.\n\n\nUpdate step: For each cluster, recompute the centroid as the mean of all points assigned to that cluster (i.e. update \\(\\mu_i\\) to be the average of points in \\(S_i\\)).\n\n\nRepeat: Iterate the assignment and update steps until convergence – typically until no points change clusters or until the centroids move negligibly. At convergence, the algorithm has reached a locally optimal partition with respect to the objective.\n\n\nThis iterative process is guaranteed to decrease the WCSS at each step and will eventually terminate (because there are a finite number of possible partitions). The output is a set of final cluster centroids and an assignment of each data point to one of the K clusters.\nMathematically, you can show that in the update step, the choice of the mean minimizes the sum of squared distances: given a cluster assignment, the best centroid (in terms of minimizing squared distance) is the arithmetic mean of the points. This justifies using the mean in step 3. Thus, K-means alternates between fixing cluster memberships (to recompute means) and fixing means (to reassign memberships), which is a special case of the more general Expectation-Maximization (EM) procedure for Gaussian mixture models. The algorithm typically converges in a few iterations for moderate-sized datasets, though it is not guaranteed to find the global minimum of the objective (it finds a local minimum, which can depend on the initialization).\n\n\nDistance Metric and Space\nDistance measure: K-means, as described, relies on Euclidean distance because the use of arithmetic mean as centroid implicitly assumes minimizing squared Euclidean distances. If one tried to use a different distance (say Manhattan \\(L_1\\) distance), the centroid (minimizer) would no longer be the arithmetic mean – in fact for \\(L_1\\) it would be the median. There are clustering variants like K-medians or K-medoids (PAM) that use other distance metrics and representative points (medoids), but standard K-means is inherently tied to Euclidean geometry.\nFeature scaling: Because it uses distance, K-means is sensitive to the scale of features. It’s common to standardize or normalize features before clustering so that one feature doesn’t dominate due to larger numerical range. For example, if one feature is in units of dollars and ranges in the thousands while another is a ratio between 0 and 1, Euclidean distance would mostly reflect differences in the dollar feature unless we scale the data.\nData types: K-means requires numeric data in a metric space. It cannot handle categorical features directly (because means and Euclidean distances aren’t defined for categories). For non-numeric data or arbitrary distance measures, other clustering algorithms (hierarchical with custom distance, density-based clustering, etc.) are more appropriate. So, K-means is best suited for continuous-valued vector data where a notion of “cluster center” as a mean makes sense.\n\n\nA Simple Demonstration in R\nLet’s walk through a demonstration of K-means in R using the classic Iris dataset. This dataset (Fisher’s iris) has 150 samples of iris flowers, measured by 4 features (sepal length, sepal width, petal length, petal width) and known species labels (setosa, versicolor, virginica). We will ignore the species labels and see if K-means can discover the natural groupings. First, we load and inspect the data, then run K-means with K=3 (since we happen to know there are 3 species, we’ll use that for illustration). We’ll also scale the features for fair distance measurement:\n\n# Load the iris dataset and scale features\ndata(iris)\niris.features &lt;- scale(iris[, -5])   # exclude the species column\n\n# Perform K-means clustering with K = 3 clusters\nset.seed(42)  # for reproducibility\nkm3 &lt;- kmeans(iris.features, centers = 3, nstart = 20)\n\n# Output the clustering result\nprint(km3)\n\nK-means clustering with 3 clusters of sizes 50, 53, 47\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  -1.01119138  0.85041372   -1.3006301  -1.2507035\n2  -0.05005221 -0.88042696    0.3465767   0.2805873\n3   1.13217737  0.08812645    0.9928284   1.0141287\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 3 3 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 3 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 47.35062 44.08754 47.45019\n (between_SS / total_SS =  76.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nSuppose the output is something like this (your numbers might differ slightly due to random initialization):\nWithin cluster sum of squares by cluster: 15.151 23.879 39.820 (between_SS / total_SS = 88.4%)\nThis tells us: the algorithm found 3 clusters with sizes 50, 38, and 62. The cluster means for each feature are listed (cluster 1 has smaller petal lengths and widths, etc., which we recognize as the setosa species characteristics, whereas clusters 2 and 3 correspond to versicolor and virginica mixes). The within-cluster sum of squares for each cluster is given (which sum up to the total WCSS), and it also reports that ~88.4% of the total variance is explained by the clustering (so the between-cluster SS is 88.4% of total SS). A high between-SS/total-SS means clusters are well-separated. This tells us: the algorithm found 3 clusters with sizes 50, 38, and 62. The cluster means for each feature are listed (cluster 1 has smaller petal lengths and widths, etc., which we recognize as the setosa species characteristics, whereas clusters 2 and 3 correspond to versicolor and virginica mixes). The within-cluster sum of squares for each cluster is given (which sum up to the total WCSS), and it also reports that ~88.4% of the total variance is explained by the clustering (so the between-cluster SS is 88.4% of total SS). A high between-SS/total-SS means clusters are well-separated.\nWe can also examine the clustering assignment versus the true species:\n\n# Compare clusters to true species\ntable(Cluster = km3$cluster, Species = iris$Species)\n\n       Species\nCluster setosa versicolor virginica\n      1     50          0         0\n      2      0         39        14\n      3      0         11        36\n\n\nCluster 1 perfectly corresponds to Setosa (50/50 setosa in that cluster). Clusters 2 and 3 split Versicolor and Virginica: cluster 2 is mostly Virginica with a few Versicolor, and cluster 3 vice versa. This reflects that Versicolor and Virginica are more similar to each other, and indeed K-means (which doesn’t know the species labels) grouped some of them together. In fact, many clustering algorithms will have trouble separating versicolor vs virginica perfectly because their measurements overlap; Setosa, on the other hand, is distinct (and K-means clearly separated it). This illustrates both the capability and the limitation of clustering: it found meaningful structure (setosa vs non-setosa) but also shows that “ground truth” classes aren’t always cleanly separable by the available features\nVisualization: We can visualize the clustered data. Since it’s 4-D, we’ll use the first two principal components for plotting. In R, we can plot the results (e.g. using autoplot(km3, data=iris.features, frame=TRUE) from the ggfortify package to plot in principal component space automatically). The clustering has partitioned the data into cohesive groups: points in the same cluster are closer to their centroid than to other centroids by construction."
  },
  {
    "objectID": "posts/kmeans-clusterdetermination-1/index.html#advantages-and-limitations-of-k-means",
    "href": "posts/kmeans-clusterdetermination-1/index.html#advantages-and-limitations-of-k-means",
    "title": "Part 1: Foundations of Clustering and K-Means",
    "section": "Advantages and Limitations of K-Means",
    "text": "Advantages and Limitations of K-Means\nK-means is popular for many reasons. Advantages include its simplicity and efficiency: the algorithm is easy to implement and understand (just loop between assignment and mean recomputation), and it’s computationally efficient, \\(O(n \\times K \\times d)\\) per iteration (where n is number of points, d is dimension). It often converges in relatively few iterations, and can handle large datasets. It also tends to produce tighter, spherical clusters which are often desirable.\nHowever, K-means has important limitations and assumptions to be aware of:\n\nYou must choose K in advance: The algorithm requires the number of clusters K as input. Choosing the “right” K is a non-trivial task (this is the focus of Part 2). Without external guidance or careful evaluation, one might under- or over-cluster the data.\nSensitivity to initialization: Different random initial centroids can lead to different final clusters (because the algorithm can get stuck in different local minima of the objective). It’s common to run K-means multiple times (the nstart parameter in R) and take the best result. Methods like K-means++ help by providing a smarter initialization that often yields better results.\nAssumes roughly spherical, equally sized clusters: K-means implicitly assumes clusters are convex and isotropic in the feature space. It tries to make clusters of comparable variance (because every cluster is represented by a centroid and distance threshold). If the true clusters are elongated, or irregularly shaped, or have hugely different sizes, K-means may perform poorly. For example, it may split one true cluster into two if it’s elongated, or merge two true clusters if one is much larger (dominates the mean) or if they’re close together. Hierarchical or density-based methods might capture such structures better.\nEuclidean distance only / mean-based: As mentioned, K-means can’t handle arbitrary distance metrics or categorical data. All features contribute squared distance to the centroid. If your data has categorical variables or you want to use a custom distance metric (like cosine similarity for text vectors), standard K-means is not applicable (though you might use K-medoids or other algorithms designed for general distances).\nSensitive to outliers: The mean is not robust – even a single extreme outlier can pull a centroid significantly, affecting the cluster. K-means will assign outliers to the nearest cluster, but that can skew the centroid and degrade clustering of other points. If your data has outliers, sometimes a preprocessing step to remove or downweight them, or using a more robust clustering (like K-medoids which uses medians), is recommended.\nNon-deterministic and local optima: Because of the random initialization and iterative improvement, K-means can end up in a suboptimal partition (a local minimum of the WCSS). In practice, repeated runs and choosing the best outcome is the standard approach to mitigate this. There are also global optimization variants and cluster ensemble methods if needed.\n**Scales poorly with very high-dimensional data*: In extremely high dimensions, distance measures become less informative (the “curse of dimensionality”). Points tend to all be nearly equidistant in high-D space, so the notion of nearest centroid gets noisy. K-means doesn’t inherently mitigate this, so it might not find meaningful clusters if d is very large unless you first apply dimension reduction (PCA, t-SNE, etc.). This isn’t unique to K-means, but it’s a general challenge for distance-based clustering.\n\nDespite these limitations, K-means often works surprisingly well on a wide variety of problems, especially when clusters are roughly spherical in feature space and well-separated. Its simplicity makes it a good baseline clustering method. In cases where its assumptions don’t hold, more advanced methods can be used, but K-means remains a workhorse for quick clustering tasks.\nAlternatives: If you encounter some of the above issues, alternatives to consider include hierarchical clustering (which can use other distances and doesn’t require a fixed K upfront), DBSCAN (density-based, finds arbitrarily shaped clusters and can detect outliers as noise), Gaussian mixture models (soft clustering using probabilistic assignments and able to model different cluster shapes via covariance), K-medoids/PAM (uses medoids and any distance, more robust to outliers), among others. We won’t dive into these here, but it’s worth noting that K-means is one tool among many."
  },
  {
    "objectID": "posts/kmeans-clusterdetermination-1/index.html#conclusion-part-1",
    "href": "posts/kmeans-clusterdetermination-1/index.html#conclusion-part-1",
    "title": "Part 1: Foundations of Clustering and K-Means",
    "section": "Conclusion (Part 1)",
    "text": "Conclusion (Part 1)\nI’ve covered the foundations of clustering and the K-means algorithm, balancing an intuitive understanding with the formal objective. K-means exemplifies how a simple iterative process can produce meaningful structure from unlabelled data, but it also illustrates key challenges like choosing the number of clusters and handling various data peculiarities. In the next part, we’ll tackle the problem we left open: how do we determine the appropriate number of clusters (K) for K-means or any clustering? This is often called the “\\(K\\) dilemma,” and as I will show, many methods have been proposed to resolve it."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html",
    "href": "posts/fwer-correction-part2/index.html",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "",
    "text": "Recall that the null hypothesis (\\(H_0\\)) is the assumption of “no effect” or “no difference.” The null distribution is the probability distribution of the test statistic under the assumption that \\(H_0\\) is true. In other words, it tells us how the test statistic would behave just by chance if the null hypothesis holds.\nFormally, if \\(T\\) is our test statistic, the null distribution is the distribution of \\(T\\) under \\(H_0\\). For example:\n\nIn the coin flip example, our test statistic was the number of heads in 20 flips. Under \\(H_0\\) (fair coin), \\(T \\sim \\text{Binomial}(20, 0.5)\\). This binomial distribution is the null distribution of \\(T\\).\nIf we test for a difference in means between two large samples under \\(H_0\\) (no true difference), the null distribution of the standardized mean difference might be approximated by a \\(t\\)-distribution or normal distribution (depending on assumptions). For instance, in a two-sample t-test with equal variances, the null distribution of the t-statistic follows a \\(t\\) distribution with appropriate degrees of freedom.\nIf testing a correlation’s significance, under \\(H_0\\) (no real correlation), a suitable transform of the correlation coefficient might follow a known distribution (like a t distribution for Pearson correlation with a given sample size).\n\nWhy is the null distribution so important? Because it provides the reference frame for significance. We compare our observed test statistic to the null distribution to see how surprising the observation is under \\(H_0\\). This comparison is at the heart of calculating p-values and determining critical values.\nSometimes we can derive the null distribution analytically from probability theory (as with the binomial or t distributions above). However, in many cases the null distribution might be complicated or unknown. In those situations, we often resort to simulation or resampling techniques (like permutation tests) to empirically estimate the null distribution."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#the-null-distribution",
    "href": "posts/fwer-correction-part2/index.html#the-null-distribution",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "",
    "text": "Recall that the null hypothesis (\\(H_0\\)) is the assumption of “no effect” or “no difference.” The null distribution is the probability distribution of the test statistic under the assumption that \\(H_0\\) is true. In other words, it tells us how the test statistic would behave just by chance if the null hypothesis holds.\nFormally, if \\(T\\) is our test statistic, the null distribution is the distribution of \\(T\\) under \\(H_0\\). For example:\n\nIn the coin flip example, our test statistic was the number of heads in 20 flips. Under \\(H_0\\) (fair coin), \\(T \\sim \\text{Binomial}(20, 0.5)\\). This binomial distribution is the null distribution of \\(T\\).\nIf we test for a difference in means between two large samples under \\(H_0\\) (no true difference), the null distribution of the standardized mean difference might be approximated by a \\(t\\)-distribution or normal distribution (depending on assumptions). For instance, in a two-sample t-test with equal variances, the null distribution of the t-statistic follows a \\(t\\) distribution with appropriate degrees of freedom.\nIf testing a correlation’s significance, under \\(H_0\\) (no real correlation), a suitable transform of the correlation coefficient might follow a known distribution (like a t distribution for Pearson correlation with a given sample size).\n\nWhy is the null distribution so important? Because it provides the reference frame for significance. We compare our observed test statistic to the null distribution to see how surprising the observation is under \\(H_0\\). This comparison is at the heart of calculating p-values and determining critical values.\nSometimes we can derive the null distribution analytically from probability theory (as with the binomial or t distributions above). However, in many cases the null distribution might be complicated or unknown. In those situations, we often resort to simulation or resampling techniques (like permutation tests) to empirically estimate the null distribution."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#the-distribution-under-h_0-vs.-under-h_1",
    "href": "posts/fwer-correction-part2/index.html#the-distribution-under-h_0-vs.-under-h_1",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "The Distribution Under \\(H_0\\) vs. Under \\(H_1\\)",
    "text": "The Distribution Under \\(H_0\\) vs. Under \\(H_1\\)\nIt’s helpful to contrast the distribution of the test statistic assuming \\(H_0\\) with its distribution assuming a particular alternative \\(H_1\\). Under \\(H_0\\), we know (or estimate) the null distribution. Under \\(H_1\\), the test statistic would typically have a different distribution – often shifted or stretched relative to the null.\nFor example, consider testing if a coin is biased toward heads. Under \\(H_0\\) (fair coin), the number of heads in 100 flips follows a binomial distribution centered at 50. Under an \\(H_1\\) where the coin is actually biased (say \\(p=0.6\\) chance of heads), the distribution of heads count would center around 60. So under \\(H_1\\), “extreme” high values of heads (say 60 or more) are in fact likely, whereas under \\(H_0\\) they are unlikely. This situation is depicted conceptually as two overlapping distributions: one (null) centered at 50, and another (alternative) centered at 60. A threshold (critical value) set at, say, 58 heads might have a low probability under \\(H_0\\) (thus controlling Type I error) while capturing a lot of the \\(H_1\\) distribution’s mass (thus giving decent power).\n\nUnder \\(H_0\\): the statistic has some known distribution (e.g., centered at 50).\nUnder \\(H_1\\): the statistic’s distribution is shifted (e.g., centered at 60 if \\(p=0.6\\)).\n\nThe power of a test is the probability of correctly rejecting \\(H_0\\) when \\(H_1\\) is true – geometrically, it’s the area of the \\(H_1\\) distribution that falls beyond the critical value (in the rejection region). We choose our test/critical value to balance controlling Type I error (false positive rate under \\(H_0\\)) and maximizing power (true positive rate under \\(H_1\\)). While \\(H_1\\) can encompass many possible values (it’s often a range of scenarios), this framework helps explain why more extreme thresholds reduce false positives but also make it harder to detect real effects.\nIn practice, we usually focus on the null distribution for calculating p-values and setting up the test, since we assume \\(H_0\\) until evidence indicates otherwise. But it’s good to remember that if there is a real effect (\\(H_1\\) true), the test statistic is expected to deviate from the null distribution systematically."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#definition-and-interpretation-of-the-p-value",
    "href": "posts/fwer-correction-part2/index.html#definition-and-interpretation-of-the-p-value",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "Definition and Interpretation of the p-value",
    "text": "Definition and Interpretation of the p-value\nNow let’s formally define the p-value. Given an observed test statistic value (call it \\(t_{\\text{obs}}\\)), the p-value is the probability of obtaining a test statistic as extreme as (or more extreme than) \\(t_{\\text{obs}}\\) assuming \\(H_0\\) is true. In formula terms:\n\nFor a right-tailed test (where large values of \\(T\\) favor \\(H_1\\)):\n\n\\[\n\\text{p-value} = \\Pr_{H_0}\\left(T \\geq t_{\\text{obs}}\\right),\n\\] the probability that \\(T\\) is greater than or equal to the observed value, under the null distribution.\n\nFor a left-tailed test (where small values favor \\(H_1\\)):\n\n\\[\n\\text{p-value} = \\Pr_{H_0}\\left(T \\leq t_{\\text{obs}}\\right).\n\\]\n\nFor a two-tailed test (where extreme in either direction counts):\n\n\\[\n\\text{p-value} = \\Pr_{H_0}\\left(\\left|T\\right| \\geq t_{\\text{obs}}\\right),\n\\] ,i.e. the probability of \\(T\\) being as far or farther from the null hypothesis expectation as the observed, in either direction. This can often be computed as twice the one-tailed probability for the tail that \\(t_{\\text{obs}}\\) lies in.\nIn less symbolic language: we locate our observed statistic on the null distribution and see what fraction (or area) of the null distribution lies at least as extreme. The more extreme \\(t_{\\text{obs}}\\) is, the smaller this tail area will be, and thus the smaller the p-value. Interpreting the p-value: If the p-value is, say, 0.03, that means “if \\(H_0\\) were true, there is a 3% chance of seeing a result as extreme as this.” A small p-value indicates the observed data are unlikely under \\(H_0\\), which is why we reject \\(H_0\\) when p is below our α threshold. Note that unlikely under \\(H_0\\) does not automatically imply \\(H_1\\) is true – it simply raises skepticism about \\(H_0\\). (It could be a rare fluke, after all; in the long run, α proportion of true nulls will yield small p-values by chance.)\nTo reinforce proper understanding, remember: The p-value is computed under the assumption that \\(H_0\\) is true. . It is not \\(P(H_0\\text{ is true} \\mid \\text{data})\\) and not \\(P(H_1\\text{ is true} \\mid \\text{data})\\). It’s about the data’s compatibility with \\(H_0\\). A low p-value says “this data would be rare if \\(H_0\\) is true,” whereas a high p-value says “this data is quite normal if \\(H_0\\) is true.”\nIf p &lt; α, we call the result statistically significant, and we reject \\(H_0\\). If p ≥ α, the result is not significant, and we do not reject \\(H_0\\).\nFor example, at α = 0.05:\n\np = 0.001 -&gt; significant (strong evidence against \\(H_0\\)),\np = 0.04 -&gt; significant (evidence against \\(H_0\\)),\np = 0.08 -&gt; not significant (data are not sufficiently unusual under \\(H_0\\)).\n\nOne more point: When test statistics are continuous, if \\(H_0\\) is true and all assumptions hold, p-values are uniformly distributed between 0 and 1. This means that under \\(H_0\\), any p-value is equally likely. As a consequence, if \\(H_0\\) is true, the probability of p ≤ 0.05 is 0.05 (which aligns with our Type I error α). This uniformity is a mathematical fact that ensures our testing procedure is calibrated. If p-values weren’t uniform under \\(H_0\\), a threshold like 0.05 would not correspond exactly to a 5% false positive rate. (This holds for continuous distributions; for discrete cases like the binomial test, p-values have a discrete distribution but still will not be biased toward low values under \\(H_0\\).)"
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#critical-values-and-the-relationship-to-p-values",
    "href": "posts/fwer-correction-part2/index.html#critical-values-and-the-relationship-to-p-values",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "Critical Values and the Relationship to P-Values",
    "text": "Critical Values and the Relationship to P-Values\nThere are two equivalent ways to conduct most hypothesis tests:\n\nCritical value approach: Determine the cutoff value of the test statistic beyond which \\(H_0\\) will be rejected. This cutoff is chosen so that if \\(H_0\\) is true, the probability the test statistic falls beyond the cutoff is α. For instance, in a right-tailed test, the critical value \\(c\\) might be the 95th percentile of the null distribution (so \\(P_{H_0}(T \\ge c) = 0.05\\)). We reject \\(H_0\\) if the observed \\(T \\ge c\\).\nP-value approach: Calculate the p-value for the observed test statistic. Reject \\(H_0\\) if p-value ≤ α.\n\nThese two approaches lead to the same conclusions. The critical value is essentially the boundary where p-value = α. For example, if the 95th percentile of the null distribution is \\(c\\), then observing \\(T=c\\) yields p-value = 0.05 (for a right-tailed test). Observing \\(T &gt; c\\) yields p &lt; 0.05, and \\(T &lt; c\\) yields p &gt; 0.05.\nExample (critical value vs p-value): Suppose \\(T \\sim N(0,1)\\) under \\(H_0\\) (a Z-test). For a two-tailed test at α = 0.05, the critical values are approximately ±1.96 (because 2.5% of the distribution is above 1.96 and 2.5% is below -1.96). So:\n\nCritical region: reject \\(H_0\\) if \\(T &gt; 1.96\\) or \\(T &lt; -1.96\\).\nIf our observed \\(T = 2.5\\), this exceeds the critical value, so we reject. The p-value would be \\(2 \\times P(Z &gt; 2.5)\\) (two-tailed) which is about 0.0124, indeed ≤ 0.05.\nIf observed \\(T = 1.0\\), this does not exceed 1.96, so we fail to reject. The p-value would be \\(2 \\times P(Z &gt; 1.0) ≈ 0.32\\), which is &gt; 0.05.\n\nIn summary, you can think of the p-value as the area in the tail beyond the observed statistic, and α as the area in the tail beyond the critical value. If the observed stat is past the critical value, the tail area beyond it (p-value) will be smaller than α."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#obtaining-null-distributions-analytical-vs-simulation",
    "href": "posts/fwer-correction-part2/index.html#obtaining-null-distributions-analytical-vs-simulation",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "Obtaining Null Distributions: Analytical vs Simulation",
    "text": "Obtaining Null Distributions: Analytical vs Simulation\nTo calculate p-values or critical values, we often need the null distribution. There are two main ways to get it:\n1.Analytical derivation (parametric approach): Using theoretical probability distributions and assumptions about the data (e.g., normality, independence), we derive the distribution of the test statistic under \\(H_0\\). Classic examples:\n\nUse the Binomial\\((n,p)\\) distribution for count of successes under \\(H_0\\).\nUse the \\(t\\) distribution for a t-statistic under \\(H_0\\) (when data are normal or \\(n\\) is large).\nUse the \\(F\\) distribution for an ANOVA F-statistic under \\(H_0\\).\nUse the \\(\\chi^2\\) distribution for a chi-square statistic under \\(H_0\\).\n\nThese derivations come from probability theory and often involve assumptions (like the data following certain distributions, or large-sample approximations). When conditions are met, the analytical approach is very convenient and fast.\n2. Simulation or resampling (nonparametric approach): When the null distribution is complex or unknown, we can simulate it. This could be:\n\nMonte Carlo simulation: Generate many random datasets under \\(H_0\\) (using a known or estimated model) and compute the test statistic for each to see its empirical distribution.\nPermutation (randomization) tests: If we have a real dataset, we can create new datasets that satisfy \\(H_0\\) by randomly shuffling or resampling the data. We then calculate the test statistic for each permuted dataset. The distribution of these statistics approximates the null distribution without relying on a formula.\n\nSimulation methods are very useful if, for example, the theoretical null distribution is unknown or too hard to derive, or if we distrust the model assumptions required for the parametric approach. They are computationally heavier but increasingly feasible with modern computing. Permutation tests (a form of simulation) are particularly powerful for preserving the exact data characteristics while enforcing \\(H_0\\) – for instance, shuffling labels in a treatment vs control experiment breaks any real difference (satisfying \\(H_0\\)), but keeps the data values themselves intact. I will cover permutation tests in detail later in the series, as they form the basis of permutation-based FWER correction.\nFor now, I will illustrate obtaining a null distribution by simulation with a simple example in R."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#example-generating-a-null-distribution-via-simulation-and-comparing-to-theory",
    "href": "posts/fwer-correction-part2/index.html#example-generating-a-null-distribution-via-simulation-and-comparing-to-theory",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "Example: Generating a Null Distribution via Simulation (and Comparing to Theory)",
    "text": "Example: Generating a Null Distribution via Simulation (and Comparing to Theory)\nConsider again the coin-flip scenario, but now with a larger number of flips to get a smoother distribution. Suppose \\(H_0\\): coin is fair (\\(p=0.5\\)), and we plan to flip \\(n=100\\) times. Our test statistic is the number of heads in 100 flips. We know analytically that under \\(H_0\\), \\(T \\sim \\text{Binomial}(100, 0.5)\\).\nLet’s obtain the null distribution in two ways: (a) analytical formula, and (b) Monte Carlo simulation of many experiments. I will then use it to compute a p-value for a specific observed outcome (say we observed 60 heads out of 100).\n\nset.seed(123)\n\n# Parameters\nn &lt;- 100          # number of coin flips\np_null &lt;- 0.5     # probability of heads under H0\nobserved &lt;- 60    # suppose this is the observed number of heads in the real experiment\n\n# (a) Analytical p-value calculation (one-tailed for illustration: H1 is bias toward heads)\np_value_theory &lt;- 1 - pbinom(observed - 1, size = n, prob = p_null)\np_value_theory  # P(X &gt;= 60) under Binomial(100, 0.5)\n\n[1] 0.02844397\n\n# (b) Simulation: generate many experiments under H0\nreps &lt;- 10000\nsim_counts &lt;- rbinom(reps, size = n, prob = p_null)  # simulate 10,000 experiments\n# Empirical p-value from simulation (fraction of experiments with &gt;= 60 heads)\np_value_sim &lt;- mean(sim_counts &gt;= observed)\np_value_sim\n\n[1] 0.0254\n\n# Check that simulation p-value ~ theoretical p-value\nabs(p_value_sim - p_value_theory)\n\n[1] 0.003043967\n\n# Visualize the null distribution from simulation\nhist(sim_counts, breaks = 20, col = \"skyblue\", main = \"Null distribution of heads count (100 flips, p=0.5)\",\n     xlab = \"Number of heads in 100 flips\")\nabline(v = observed, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nLet’s break down what I did:\n\nI set observed &lt;- 60 as an example outcome (60 heads out of 100 flips).\nThe theoretical one-tailed p-value (for the hypothesis of bias towards more heads) is computed as 1 - pbinom(59, 100, 0.5). This gives \\(P(X \\ge 60)\\) for \\(X \\sim \\text{Binomial}(100,0.5)\\).\nFinally, I plotted a histogram of the simulated null distribution and drew a red line at the observed value 60.\n\nIf you run this, you should see that p_value_theory and p_value_sim are very close (they’ll differ by a small random error on the order of \\(10^{-3}\\) or so). For instance, you might get something like p_value_theory ≈ 0.0284 and p_value_sim ≈ 0.0291 – these are essentially the same, confirming that our simulation is accurately capturing the binomial distribution.\nEmpirical null distribution of the test statistic (number of heads in 100 flips) under the \\(H_0\\) that the coins are exactly fair: The histogram shows the outcomes of 10,000 simulated fair coin experiments, each with 100 flips. The distribution is roughly bell-shaped, centered around near 50 heads. The red line marks the observed result (60 heads) from a single experiment. The p-value for the observation is the proportion of the null distribution that lies to the right of this red line (the shaded area beyond 60). In this example, the p-value is about 0.03, meaning there is a ~3% chance of seeing 60 or more heads if the coin is truly fair (or, more specifically, if the true expected value of the experiment is 50).\nAs expected, the null distribution is centered at 50 (when the coin is fair, on average half the flips are heads). Getting 60 heads is somewhat uncommon under \\(H_0\\) (only ~3% of random fair-coins flips achieved that or more in our simulation). If we were testing \\(H_0\\) at α = 0.05 with the alternative hypothesis “biased toward heads” (one-tailed), we would reject \\(H_0\\) since p ≈ 0.03 &lt; 0.05. If our alternative was two-tailed (detecting any bias), we would consider both tails (≤40 heads or ≥60 heads) which roughly doubles the p-value to ~0.06; in that two-tailed case, 60 heads is actually borderline and would not be significant at the 0.05 level (because while 60 is unusual, so would an equally extreme deficit of heads – our observed outcome is only extreme on the high side).\nThis example illustrates how we use null distributions to obtain p-values, and how simulation can validate or substitute for analytical calculations. In more complex situations where we can’t write down a formula for the null distribution, we can rely on simulation or permutation to approximate it. The histogram also gives a visual intuition: most outcomes cluster around 50; 60 is out in the tail (hence p ~ 0.03 is the area of that tail)."
  },
  {
    "objectID": "posts/fwer-correction-part2/index.html#the-role-of-null-distributions-in-multiple-comparisons",
    "href": "posts/fwer-correction-part2/index.html#the-role-of-null-distributions-in-multiple-comparisons",
    "title": "Permutation-Based FWER Correction, Part 2: Background pt 2",
    "section": "The Role of Null Distributions in Multiple Comparisons",
    "text": "The Role of Null Distributions in Multiple Comparisons\n(A brief forward-looking note): When we perform multiple hypothesis tests (common in fields like neuroimaging, where we often test thousands of voxels for activation differences), the null distribution of the maximum test statistic or other aggregate becomes important for controlling the family-wise error rate. Permutation methods often involve simulating the null distribution for the entire set of tests by shuffling data labels, and then using the distribution’s quantiles to define corrected critical values that ensure the overall Type I error (FWER) is controlled. In later posts, we will see how the fundamentals from Part 1 and Part 2 come together to address those scenarios. Specifically, we will use permutation-generated null distributions to adjust for multiple comparisons (FWER), ensuring that the chance of any false positive across many tests stays at a desired level (like 5%).\nUnderstanding null distributions and p-values is a critical step in that journey. You should now have a solid grasp of what p-values represent, how to obtain them, and how to interpret them in the context of hypothesis tests. In the next part of this series, I will delve into multiple testing and error rates (including FWER and the false discovery rate) and introduce permutation-based approaches to control these error rates. I’ll show how permutation tests can be applied in a neuroscience context (e.g. analyzing EEG or fMRI data) to make robust inferences while accounting for the multiplicity of comparisons."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This page will eventually describe my research interests, collaborations, and contact details."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "",
    "text": "I am a clinical data scientist in the Sleep, Cognition and Neuropsychology lab at Massachusetts General Hospital and a postdoctoral research fellow at Harvard Medical School.\nMy expertise is in statistical and computational analysis of health-related data, with a focus on neuroimaging and electrophysiological signals in neuropsychiatric disorders. I create and maintain validated, reproducible analysis pipelines and develop advanced statistical methods to support translational research and biomarker discovery. You can read up on some of my methods development work in anatomically guided MEG/EEG sleep spindle detection and resolving source localization disagreement in resting-state networks, or my work on the relationship between cortical network disruption and cognitive impairment in multiple sclerosis, how sleep spindles topography relates to memory encoding and consolidation, or how hippocampal ripples facilitate wake offline learning.\nI received a PhD in Biomedical and Pharmaceutical Sciences at the Université libre de Bruxelles and hold a master’s degree in Neuroscience and a bachelor’s degree in Human Movement Science, both from the Norwegian University of Science and Technology."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "",
    "text": "I am a clinical data scientist in the Sleep, Cognition and Neuropsychology lab at Massachusetts General Hospital and a postdoctoral research fellow at Harvard Medical School.\nMy expertise is in statistical and computational analysis of health-related data, with a focus on neuroimaging and electrophysiological signals in neuropsychiatric disorders. I create and maintain validated, reproducible analysis pipelines and develop advanced statistical methods to support translational research and biomarker discovery. You can read up on some of my methods development work in anatomically guided MEG/EEG sleep spindle detection and resolving source localization disagreement in resting-state networks, or my work on the relationship between cortical network disruption and cognitive impairment in multiple sclerosis, how sleep spindles topography relates to memory encoding and consolidation, or how hippocampal ripples facilitate wake offline learning.\nI received a PhD in Biomedical and Pharmaceutical Sciences at the Université libre de Bruxelles and hold a master’s degree in Neuroscience and a bachelor’s degree in Human Movement Science, both from the Norwegian University of Science and Technology."
  },
  {
    "objectID": "index.html#featured-publications",
    "href": "index.html#featured-publications",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "Featured Publications",
    "text": "Featured Publications\n\nHippocampal ripples predict motor learning during brief rest breaks in humans (2025)\nMartin Sjøgård*, Bryan Bazter, Dimitrios Mylonas, Megan Thompson, Kristi Kwok, Bailey Driscoll, Anabella Tolosa, Wen Shi, Robert Stickgold, Mark Vangel, Catherine J. Chu, Dara S. Manoach. Nature Communications*, 16(6089)\nBrain dysconnectivity relates to disability and cognitive impairment in multiple sclerosis (2021)\nMartin Sjøgård*, Vincent Wens, Jeroen Van Schependom, Lars Costers, Marie D’hooghe, Miguel D’haeseleer, Mark Woolrich, Serge Goldman, Guy Nagels, Xavier De Tiège. Human Brain Mapping*, 42(3): 626–643\nDo the posterior midline cortices belong to the electrophysiological default-mode network? (2019)\nMartin Sjøgård*, Xavier De Tiège, Alison Mary, Philippe Peigneux, Serge Goldman, Guy Nagels, Jeroen Van Schependom, Andrew J Quinn, Mark W Woolrich, Vincent Wens. NeuroImage, 200: 221–230\nA novel approach to estimating the cortical sources of sleep spindles using simultaneous EEG/MEG (2022)\nDimitrios Mylonas*, Martin Sjøgård*, Zhaoyue Shi, Bryan Baxter, Matti Hämäläinen, Dara S Manoach, Sheraz Khan. Frontiers in Neurology, 13:871166"
  },
  {
    "objectID": "index.html#quick-paper-summaries",
    "href": "index.html#quick-paper-summaries",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "Quick paper summaries",
    "text": "Quick paper summaries\nWant a quick summary of what I’ve been working on? Interested in my research but don’t have time to read complicated and long papers? Are you a family member who’s interested but not that interested? A recruiter wanting to see what I’ve done? Look no further, these quick summaries of my papers will give you an overview without all the details. If you want all the details, the link to the full paper is inside.\n\n**Increased sleep spindles in regions engaged during motor learning predict memory consolidation\nHippocampal ripples predict motor learning during brief rest breaks in humans (2025)"
  },
  {
    "objectID": "index.html#featured-blog-posts",
    "href": "index.html#featured-blog-posts",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "Featured Blog Posts",
    "text": "Featured Blog Posts\n\nPower Analysis for Linear Mixed Models\nPermutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals\nPermutation-Based FWER Correction, Part 2: Adjusted P-Values and Inference with Permutation-Based Correction\nPermutation-Based FWER Correction, Part 7: Adjusted P-Values and Inference with Permutation-Based Correction\nDeveloping a novel cluster number determination scheme for K-means clustering"
  },
  {
    "objectID": "index.html#recently",
    "href": "index.html#recently",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "Recently",
    "text": "Recently\n\nMost learning on a motor task happens during the short periods of rest between trials (i.e. ‘offline’ learning). Do we also see higher hippocampal ripple rates during these periods? Do these hippocampal ripples correlate with the amount of offline learning? Spoiler alert! Our most recent paper was just published in Nature Communications"
  },
  {
    "objectID": "index.html#upcoming-and-recent-talks",
    "href": "index.html#upcoming-and-recent-talks",
    "title": "Martin Sjogard | Neuroscience, Data Science, and Research",
    "section": "Upcoming and Recent Talks",
    "text": "Upcoming and Recent Talks\n\nDisrupted ipsilateral encoding of motor sequence learning in Schizophrenia: spectral and hemispheric aspects, talk, SCAN group, May 20, 2025, Boston, MA, USA.\nSchizophrenia patients learn normally but do not consolidate: A case for differential sleep spindle topography, talk, MEG methods group, Jan 10, 2025, Boston, MA, USA.\nSleep spindle increases follow cortical patterns of task encoding, talk, MEG methods group, Jan 10, 2024, Boston, MA, USA.\nSleep spindle increases and Task learning vs Sleep-dependent memory consolidation: Two topographically distinct relationships, talk, SCAN group, Aug 15, 2023, Boston, MA, USA.\nHippocampal ripples and memory, talk, Masachusetts General Hospital, Aug 1, 2023, Boston, MA, USA."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html",
    "href": "posts/fwer-correction-part1/index.html",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "",
    "text": "I very commonly get questions about nonparametric statistical control from collaborators, and I have been maintaining some personal notes that I tend to distribute to them on request. Here, I have modified them significantly with the intention of making them readable for the younger trainees who also ask me about these things. It should (hopefully) be readable for my collaborators of any background (psychology undergrads, physicians, engineers, neuroscientists).\nI will periodically update it with more of my notes. The notes start with establishing some common understanding of hypothesis testing, the concept of a test statistic, distributions of a repeated statistic under the null, and the need for thresholding, then walks (or rambles) through multiple comparisons as a concept, various resampling approaches, some permutation intuition, and finally to the permutation-based family-wise error rate correction and the maximum statistic. The goal is to have the reader think about how distributions are generated under specific constraints and assumptions, and how thresholding these distributions directly relate to the questions we’re asking of the data. I find that these permutation-based/non-parametric concepts manage what standard courses on parametric statistics taught in every psychology stats class never does: to instill statistical intuition in people.\nBefore diving into permutation tests and error correction, it’s essential to establish a strong foundation in basic statistical inference. In this first post, I will cover the fundamentals of hypothesis testing: what hypotheses are, how we use test statistics, the meaning of Type I errors and significance levels, what it means to “reject \\(H_0\\),” and an intuitive introduction to p-values. I will also walk through a simple example (with R code) to illustrate these concepts in practice."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#null-and-alternative-hypotheses",
    "href": "posts/fwer-correction-part1/index.html#null-and-alternative-hypotheses",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "Null and Alternative Hypotheses",
    "text": "Null and Alternative Hypotheses\nA statistical hypothesis is a claim or conjecture about a population or process that can be tested with data. In hypothesis testing, we usually have two competing hypotheses:\n\nNull hypothesis (\\(H_0\\)): This is the default or status quo hypothesis that there is no effect or no difference. It often represents a baseline assumption or “no change” scenario.\nAlternative hypothesis (\\(H_1\\) or \\(H_A\\)):: This is the hypothesis that there is an effect, a difference, or a deviation from the null. It typically represents what we are trying to find evidence for.\n\nThese two hypotheses are complementary: if one is true, the other is false. We design a test to decide whether the data provide sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\). Importantly, we begin by assuming \\(H_0\\) is true (akin to “innocent until proven guilty” in a trial) and then ask whether the data are unlikely enough under that assumption to warrant rejecting it. The alternative hypothesis can be one-sided (e.g., an increase in mean, but not a decrease) or two-sided (any difference, either an increase or decrease).\nA general example: Suppose we are testing a new drug.\n\n\\(H_0\\): The drug has no effect on blood pressure (mean change = 0).\n\\(H_1\\): The drug does have an effect (mean change ≠ 0).\n\nA neuroimaging example: In a brain imaging study, we might test if a particular brain region is more active during a task than at rest.\n\n\\(H_0\\): No difference in brain activity between task and rest in that region.\n\\(H_1\\): A significant difference in activity exists between task and rest.\n\nIn both cases, \\(H_0\\) posits “no change/effect,” and \\(H_1\\) posits “some change/effect.” The goal of the test is not to prove \\(H_0\\) (we generally assume \\(H_0\\) true until evidence shows otherwise), but rather to determine if we have enough evidence to reject \\(H_0\\) and infer \\(H_1\\)."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#test-statistic",
    "href": "posts/fwer-correction-part1/index.html#test-statistic",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "Test Statistic",
    "text": "Test Statistic\nTo carry out a hypothesis test, we need a test statistic – a numerical summary of the data that we can use to decide between \\(H_0\\) and \\(H_1\\). The test statistic is chosen based on the problem and hypotheses; it should capture the effect we are looking for. Common examples include:\n\nThe difference between two group means (for testing if two groups differ).\nA proportion or count of “successes” (for testing rates or probabilities, like the number of heads in coin flips).\nA correlation coefficient (for testing association between variables).\n\nThe key property of a test statistic is that we know (or can determine) its probability distribution when \\(H_0\\) is true. This distribution under the null hypothesis is called the null distribution of the test statistic. Knowing the null distribution allows us to quantify how extreme our observed statistic is, assuming \\(H_0\\) is correct. For example, if our test statistic is the number of heads in 100 coin flips and \\(H_0\\) states the coin is fair, the null distribution of the statistic is a Binomial(100, 0.5) distribution (the distribution of heads counts for a fair coin).\nWhy do we need a test statistic? It provides a standardized way to evaluate evidence. Raw data can be messy, but a well-chosen statistic (like a mean difference or a \\(t\\)-value) condenses the information into a single number that we can compare against a reference distribution. If the observed test statistic falls in a range that would be very unusual under \\(H_0\\) (e.g., far in the tail of the null distribution), it suggests the data are not consistent with \\(H_0\\)."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#type-i-error-and-significance-level",
    "href": "posts/fwer-correction-part1/index.html#type-i-error-and-significance-level",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "Type I Error and Significance Level",
    "text": "Type I Error and Significance Level\nWhen making decisions based on data, we must acknowledge the possibility of errors. In hypothesis testing, there are two fundamental error types:\n\nType I error (false positive): Rejecting the null hypothesis when it is actually true. In other words, a false alarm – we think we found an effect, but there is none. This is analogous to convicting an innocent person in a trial.\nType II error (false negative): Failing to reject the null hypothesis when the alternative is true. In other words, a miss – there is a real effect, but our test failed to detect it. Analogously, a guilty person goes free due to lack of evidence.\n\nBecause we never have absolute certainty (we rely on samples of data), we cannot eliminate these errors entirely. Instead, we control their probabilities. The significance level of a test, denoted \\(\\alpha\\), is the probability of making a Type I error that we are willing to tolerate. Common choices are \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%). For example, \\(\\alpha = 0.05\\) means that if \\(H_0\\) is true, we are willing to accept a 5% chance of mistakenly rejecting it (5% false positive rate). In practical terms, \\(\\alpha=0.05\\) means that if \\(H_0\\) is true, the kind of result we deem “significant” would be expected less than 5% of the time by random chance. Note! This is a common way to talk about these values in general but not strictly correct. When I discuss distributions and thresholds more later I will point out the issues with this phrasing.\nThe significance level defines a threshold for significance: if the evidence against \\(H_0\\) is strong enough that the probability of seeing such evidence under \\(H_0\\) is less than \\(\\alpha\\), we will reject \\(H_0\\). This threshold can be equivalently thought of in terms of critical values of the test statistic (more on that later) or p-values (coming up next).\nIt’s important to note that \\(\\alpha\\) is set by the researcher before seeing the data. This prevents bias in deciding what is “significant.” The smaller \\(\\alpha\\) is, the more stringent the test (lower chance of false positive, but higher chance of missing a real effect, i.e., higher Type II error). In the context of multiple comparisons (many tests at once, as often occurs in neuroscience with many brain measurements), controlling the family-wise error rate is essentially about managing the overall Type I error – we’ll get to that in a later post.\nSide note: Continuing the courtroom analogy – if we treat “innocent until proven guilty” as \\(H_0\\), a Type I error is convicting an innocent person, and we set a high bar (stringent \\(\\alpha\\)) to avoid that. A Type II error is letting a guilty person go free. The significance level is like the “beyond a reasonable doubt” threshold: we decide how much evidence is enough to reject innocence. In hypothesis testing, \\(\\alpha\\) is our chosen threshold for doubt. For example, using \\(\\alpha = 0.01\\) is like requiring very strong evidence to convict, minimizing false convictions (Type I errors) at the expense of potentially acquitting more truly guilty defendants (some increased Type II errors)."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#rejecting-h_0-what-does-it-mean",
    "href": "posts/fwer-correction-part1/index.html#rejecting-h_0-what-does-it-mean",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "Rejecting \\(H_0\\): What Does it Mean?",
    "text": "Rejecting \\(H_0\\): What Does it Mean?\nWhen we say “reject \\(H_0\\)”, it means the sample data are sufficiently inconsistent with \\(H_0\\) that we decide \\(H_0\\) is unlikely to be true. In practice, rejecting \\(H_0\\) implies accepting \\(H_1\\) (or at least concluding that there is evidence in favor of \\(H_1\\)). A result that leads to rejection is often called “statistically significant” at the chosen \\(\\alpha\\) level. This does not mean \\(H_1\\) is proven true in all cases, only that \\(H_0\\) is implausible given the data. We make this decision with a controlled Type I error rate (if \\(H_0\\) is true, we’ll only mistakenly reject it \\(\\alpha\\) fraction of the time in repeated experiments).\nConversely, if we “fail to reject \\(H_0\\),” it means the data did not provide strong enough evidence against \\(H_0\\). Important: Failing to reject is not the same as accepting \\(H_0\\) as true – it simply means we do not have sufficient evidence to say it’s false. It’s possible that \\(H_0\\) is false but our sample was not extreme enough (this would be a Type II error). Thus, “not significant” does not prove the null, it only indicates lack of evidence to reject it.\nIn summary:\n\nReject \\(H_0\\) (Significant result): Data are unlikely under \\(H_0\\) (p-value ≤ α). We infer evidence for \\(H_1\\).\nDo not reject \\(H_0\\) (Non-significant): Data are not sufficiently unusual under \\(H_0\\) (p-value &gt; α). We cannot conclude \\(H_1\\); \\(H_0\\) remains plausible.\n\nThis decision process ensures a controlled false positive rate. If \\(H_0\\) is actually true, the chance we incorrectly reject it is α (by design). For example, with α = 0.05, over many repeated experiments with true nulls, about 5% would yield (erroneous) significant results just by chance."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#an-intuitive-introduction-to-p-values",
    "href": "posts/fwer-correction-part1/index.html#an-intuitive-introduction-to-p-values",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "An Intuitive Introduction to p-Values",
    "text": "An Intuitive Introduction to p-Values\nUp to now, I’ve talked about “evidence” and outcomes being “unlikely under \\(H_0\\).” The tool that quantifies this is the p-value. We will give a precise definition in the next post, but conceptually: The p-value is a measure of how surprising the observed data would be if the null hypothesis were true. It answers the question: “If \\(H_0\\) is true, what is the probability of obtaining a result at least as extreme as what we observed?”\nA high p-value (say 0.5) means our result is quite ordinary under \\(H_0\\) (nothing surprising). A very low p-value (say 0.001) means our result would be very rare if \\(H_0\\) were true, indicating that either we saw a fluke or, more plausibly, \\(H_0\\) is false. Some intuitive points about p-values:\n\nSmall p-value = evidence against \\(H_0\\): If the data would hardly ever occur under \\(H_0\\), we have reason to doubt \\(H_0\\). For example, getting 95 heads out of 100 coin flips (p-value extremely small under a fair-coin hypothesis) strongly suggests the coin is not fair.\nModerate/large p-value = consistency with \\(H_0\\): If the data are fairly typical under \\(H_0\\), we have no grounds to reject it. E.g., 52 heads out of 100 (p ≈ 0.39 for a fair coin) is quite a common outcome — nothing suspicious.\nThreshold comparison: We compare the p-value to α. If p ≤ α, the result is statistically significant (reject \\(H_0\\)); if p &gt; α, it’s not significant (fail to reject \\(H_0\\)). This is equivalent to the critical-value approach but more directly answers the “how rare is this result under \\(H_0\\)?” question.\nOne-tailed vs Two-tailed: The p-value calculation depends on \\(H_1\\). If \\(H_1\\) is one-sided (e.g., an increase in mean), we calculate the probability of results as extreme as the observed in that direction. If \\(H_1\\) is two-sided (any difference), we consider extremeness in both directions (e.g., both high and low extremes). Two-tailed tests essentially double-count the tail area corresponding to the observed result’s extremeness.\n\nA common misconception is that the p-value is the probability that \\(H_0\\) is true given the data. This is NOT what the p-value represents. The p-value is calculated under the assumption that \\(H_0\\) is true; it is not the probability of \\(H_0\\) itself. It also isn’t the probability that \\(H_1\\) is true. Instead, it’s about the data: assuming no real effect (\\(H_0\\)), how surprising is what we observed? To cement these ideas, let’s walk through a simple example and see how to compute a p-value and make a decision."
  },
  {
    "objectID": "posts/fwer-correction-part1/index.html#example-testing-a-coin-for-fairness-with-r-code",
    "href": "posts/fwer-correction-part1/index.html#example-testing-a-coin-for-fairness-with-r-code",
    "title": "Permutation-Based FWER Correction, Part 1: Hypothesis Testing Fundamentals",
    "section": "Example: Testing a Coin for Fairness (with R code)",
    "text": "Example: Testing a Coin for Fairness (with R code)\nImagine you have a coin and you suspect it might be biased (not fair). We can frame this as a hypothesis test:\n\n\\(H_0\\): The coin is fair (probability of heads \\(p = 0.5\\)).\n\\(H_1\\): The coin is not fair (\\(p \\neq 0.5\\), two-sided alternative).\n\nWe decide to flip the coin \\(n=20\\) times and count the number of heads. Our test statistic will be the number of heads in 20 flips. Under \\(H_0\\) (fair coin), the null distribution of this statistic is \\(\\text{Binomial}(n=20, p=0.5)\\). Let’s say we perform the experiment. Below is some R code to simulate the coin flips and conduct the test:\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define number of flips\nn &lt;- 20\n\n# Simulate 20 coin flips (H = \"heads\", T = \"tails\")\nflips &lt;- sample(c(\"H\", \"T\"), size = n, replace = TRUE, prob = c(0.5, 0.5))\nflips  # show the sequence of flips\n\n [1] \"H\" \"H\" \"T\" \"H\" \"H\" \"H\" \"H\" \"T\" \"H\" \"H\" \"T\" \"H\" \"H\" \"T\" \"T\" \"H\" \"H\" \"T\" \"T\"\n[20] \"H\"\n\n# Count the number of heads\nnum_heads &lt;- sum(flips == \"H\")\nnum_heads  # print the number of heads observed\n\n[1] 13\n\n# Perform a two-sided test for fairness:\n# H0: p = 0.5, H1: p != 0.5\ntest_result &lt;- binom.test(num_heads, n, p = 0.5, alternative = \"two.sided\")\ntest_result\n\n\n    Exact binomial test\n\ndata:  num_heads and n\nnumber of successes = 13, number of trials = 20, p-value = 0.2632\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4078115 0.8460908\nsample estimates:\nprobability of success \n                  0.65 \n\n\nExplanation: I used sample() to simulate 20 flips of a fair coin. I then count heads. The function binom.test() performs an exact binomial test for the null hypothesis \\(p=0.5\\). The output of binom.test will include a p-value.\nIf you run this code, you’ll get a certain number of heads (because of randomness, it could be different each run). For instance, one simulation might yield num_heads = 12 out of 20. In that case, the test result will show a p-value (for 12 heads, p ≈ 0.503). This high p-value indicates 12/20 heads is very consistent with a fair coin (no evidence against \\(H_0\\)). We would fail to reject \\(H_0\\) at α = 0.05.\nNow, to see what a significant result would look like, let’s consider a more extreme outcome. Imagine we had observed num_heads = 17 out of 20. Intuitively, 17 heads in 20 flips is quite unlikely if the coin is fair. We can calculate the p-value for 17 heads:\n\n# Compute p-value for observing 17 or more heads out of 20 if p=0.5\np_val &lt;- sum(dbinom(17:20, size = 20, prob = 0.5))\np_val\n\n[1] 0.001288414\n\n\nThis sums the probabilities of getting 17, 18, 19, or 20 heads under a fair coin (two-tailed test would also include the equally unlikely lower tail: 0–3 heads). The resulting one-tail probability for ≥17 heads is very small (around 0.0013), and doubling for two tails gives ~0.0026. Indeed, binom.test(17, 20, 0.5) would return p ≈ 0.0026. This p-value is far below 0.05, so such an outcome would be deemed highly significant. We would reject \\(H_0\\) and conclude the coin is likely biased.\nTo summarize the coin test example:\n\nIf we observe a moderate number of heads (say 8, 10, 12, etc.), the p-value is high (well above 0.05). We do not have evidence to reject \\(H_0\\); the coin could well be fair.\nIf we observe an extreme number of heads (like 17 out of 20), the p-value is very low (≪ 0.05). This is significant evidence against \\(H_0\\); we reject fairness and suspect the coin is biased.\nOur test controlled the Type I error at 5%. If the coin were actually fair, there’s only a 5% chance that we’d see a result as extreme as to falsely conclude bias.\n\nThis simple example highlights how hypothesis testing works in practice: define \\(H_0\\) and \\(H_1\\), choose a test statistic and significance level, collect data, compute a p-value (or compare to a critical value), and decide whether or not to reject \\(H_0\\). We used a p-value approach here (directly computing the probability of the observed outcome under \\(H_0\\)). Alternatively, one could use a critical value approach: for instance, for α = 0.05 in the two-sided coin test, the critical region would be ≤3 heads or ≥17 heads (the most extreme 5% of the null distribution). Indeed, 17 heads was the cutoff — that’s why 17 gave p ~0.0026 (half of the 5% two-tailed region)."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html",
    "href": "posts/kmeans-permutation/index.html",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "",
    "text": "I have been thinking about novel ways to estimate the number of clusters in K-means clustering for a few weeks. Of the available methods, many provide a heuristic or relative measure of cluster quality, but few give a firm statistical significance test for the presence of clusters. In this part, I will walk through a porposed approach – a Permutation-Based Permutation Test on within-cluster-sum-of-squares (WCSS) – to determine the number of clusters in a more hypothesis-driven manner. The core idea is to ask: “Is the reduction in within-cluster variance achieved by having K clusters significantly better than what would be expected by random chance?” If yes, then K clusters captures real structure; if not, adding that cluster may be fitting noise.\nThis method is inspired by permutation tests commonly used in statistical inference and by recent research that applies permutation strategies to clustering. I’ll leverage permutations to create a baseline for clustering performance under a null hypothesis of no meaningful clusters. I’ll start with the null hypothesis that the data have no intrinsic cluster structure (i.e., no or one cluster, depending on your view). I then measure how much WCSS improves increase K increases, and I compare that to how much improvement you could get on random shuffled data. When the improvement is no longer significant, I stop increasing K. This yields an estimated “optimal” K with a statistical justification (controlled false-alarm rate). This approach combines intuitive reasoning (clustering should significantly outperform random partitions) with rigorous testing.\nI thought about this approach for about one week so far, so it may well be trash. Let’s see where I end up:"
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#motivation-and-intuition",
    "href": "posts/kmeans-permutation/index.html#motivation-and-intuition",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "",
    "text": "I have been thinking about novel ways to estimate the number of clusters in K-means clustering for a few weeks. Of the available methods, many provide a heuristic or relative measure of cluster quality, but few give a firm statistical significance test for the presence of clusters. In this part, I will walk through a porposed approach – a Permutation-Based Permutation Test on within-cluster-sum-of-squares (WCSS) – to determine the number of clusters in a more hypothesis-driven manner. The core idea is to ask: “Is the reduction in within-cluster variance achieved by having K clusters significantly better than what would be expected by random chance?” If yes, then K clusters captures real structure; if not, adding that cluster may be fitting noise.\nThis method is inspired by permutation tests commonly used in statistical inference and by recent research that applies permutation strategies to clustering. I’ll leverage permutations to create a baseline for clustering performance under a null hypothesis of no meaningful clusters. I’ll start with the null hypothesis that the data have no intrinsic cluster structure (i.e., no or one cluster, depending on your view). I then measure how much WCSS improves increase K increases, and I compare that to how much improvement you could get on random shuffled data. When the improvement is no longer significant, I stop increasing K. This yields an estimated “optimal” K with a statistical justification (controlled false-alarm rate). This approach combines intuitive reasoning (clustering should significantly outperform random partitions) with rigorous testing.\nI thought about this approach for about one week so far, so it may well be trash. Let’s see where I end up:"
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#intuitive-explanation-to-sort-my-own-thoughts",
    "href": "posts/kmeans-permutation/index.html#intuitive-explanation-to-sort-my-own-thoughts",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Intuitive explanation to sort my own thoughts",
    "text": "Intuitive explanation to sort my own thoughts\nImagine you have a cloud of data points. Now suppose I randomly assign them into K clusters (just by chance, without regard to their positions). Even with random assignment, you’ll get some WCSS value – probably high (clusters won’t be tight) but not as high as one cluster because random assignment might by chance put some close points together. If you try different random assignments, you get a distribution of WCSS values that represent “what WCSS might look like if there were no real clusters, just random groupings.”\nNow, if your actual K-means clustering on the real data yields a WCSS that is much lower than these random assignments, that suggests the data has genuine cluster structure (points are much closer to their cluster mates than random chance would allow). I can formalize this by a permutation test: shuffle the data (or equivalently, break any structure in data by permuting) and recompute clustering many times to see what random WCSS values I get. Then check where the actual WCSS lies in that distribution.\nFor a given K, the null hypothesis H0 is: “there is no clustering structure (points are i.i.d. with no clusters)”. If H0 were true, any grouping of that size K is essentially arbitrary. The alternative is the data has an inherent K-cluster structure that produces a significantly lower WCSS than random groupings.\nI will find the largest K for which I can reject H0 (i.e., the clustering is significantly better than random). That K will be my chosen number of clusters. Intuitively, as K grows large, even random assignments can achieve low WCSS (because many clusters = each cluster has few points = trivially small within-cluster variance). So at some point, the real data’s advantage over random will diminish. I will want to stop at the point where adding another cluster no longer yields a statistically significant gain. This approach reduces subjectivity by anchoring cluster decisions to a significance level (like 0.05). It’s like saying “I’ll only increase K as long as each new cluster gives me a meaningful improvement beyond what noise could explain.”"
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#statistical-foundation",
    "href": "posts/kmeans-permutation/index.html#statistical-foundation",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Statistical foundation",
    "text": "Statistical foundation\nI define \\(W(K)\\) as the total within-cluster sum of squares for K clusters (computed by, say, K-means algorithm). Note \\(W(1) \\ge W(2) \\ge \\dots \\ge W(n)\\) (monotonically decreases as K increases). I want to test for each K &gt; 1 whether the decrease from K-1 to K is significant.\nOne way to do this is consider the drop in WCSS when going from K-1 to K clusters: \\(\\Delta W_K = W(K-1) - W(K)\\). This is the reduction in within-cluster variance by adding one more cluster. If data have K real clusters, I’d expect a big drop in WCSS up to K, then much smaller drops after (elbow effect). I can test:\n\nH0: Data has at most K-1 clusters (no meaningful K cluster structure). Under H0, adding the Kth cluster yields little improvement beyond random chance.\nH1: Data has K clusters (the Kth cluster captures structure).\n\nI simulate H0 by permutation: a simple method is to randomly permute each feature independently among data points (this destroys any multivariate structure or clustering, while roughly preserving univariate distributions). Another is to randomly assign cluster labels to points (which is akin to random grouping). Permuting features is nice because it keeps the distribution of each coordinate but breaks correlations that define cluster separation\nFor each permutation \\(b\\) (b = 1..B), I compute \\(W^{(b)}(K)\\) using K-means on the permuted data. This gives a null distribution of WCSS for K clusters. I can compute \\(\\Delta W_K^{(b)} = W^{(b)}(K-1) - W^{(b)}(K)\\) for the permuted data as well. Now consider a test statistic. I could use the drop \\(\\Delta W_K\\) directly, or an F-like ratio. The Calinski-Harabasz index is essentially proportional to \\(\\frac{\\Delta W_K/(K-1)}{W(K)/(,n-K,)}\\), which looks like an F. But for simplicity, I can just use \\(\\Delta W_K\\) itself or \\(W(K)\\) itself. A simple choice: use \\(W(K)\\) as the test statistic (lower is better clustering). If the data truly has K clusters, \\(W_{\\text{real}}(K)\\) will be much lower than \\(W^{(b)}(K)\\) from permuted data. So I can calculate a p-value:\n\\[\np_K = \\Pr\\left(W^{\\text{perm}}(K) \\leq W_{\\text{real}}(K)\\right)\n\\]\nIn practice, from B permutations,\n\\[\n\\hat{p}_K = \\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{I} \\left\\{ W^{(b)}(K) \\leq W_{\\text{real}}(K) \\right\\},\n\\]\ni.e. the fraction of permuted trials where the permuted WCSS is as low or lower than the actual. If \\(p_K\\) is very small, it means actual data’s clustering is much tighter than any random permutation achieved – evidence for K clusters.\nHowever, using \\(W(K)\\) directly can be problematic because \\(W(K)\\) always decreases with K even for permuted data. Instead, focusing on the improvement from K-1 to K might be more informative. So alternatively, test whether \\(\\Delta W_K\\) is larger than expected under null. Formally:\n\\[\np_K = \\Pr\\left( \\Delta W_K^{\\text{perm}} \\geq \\Delta W_K^{\\text{real}} \\right)\n\\]\nI want to see if the drop in WCSS by adding cluster K is bigger than what random data would typically give when adding one more cluster. The two approaches (testing \\(W(K)\\) or testing \\(\\Delta W_K\\)) are related. Testing \\(W(K)\\) directly asks if the clustering as a whole is unusual; testing \\(\\Delta W_K\\) focuses on the incremental benefit of the Kth cluster. The incremental approach might be better to decide when to stop adding clusters sequentially.\nProcedure outline: Start at K=1. Increase K one by one, and for each K do a permutation test: if significant (say p &lt; 0.05), reject H0 that no K-cluster structure, and continue; if not significant, stop – use K-1 as final clusters. This sequential testing needs a little care with multiple comparisons, but one can control overall type I error by a small alpha at each step or use a stepwise stopping rule (similar to how forward selection in stats stops when no significant improvement). This method essentially ensures I only add clusters while they provide a statistically significant improvement. Permutation approaches have been used in hierarchical clustering to decide where to cut a dendrogram, and multi-aspect tests combine multiple indices for a global test. Here I focus on WCSS since it’s natural for K-means."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#algorithm-description",
    "href": "posts/kmeans-permutation/index.html#algorithm-description",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Algorithm description",
    "text": "Algorithm description\nPutting it all together, here’s the algorithm for the permutation test method:\n\n1. Input: Data matrix X (n x d), maximum clusters K_max to consider, number of permutations B, significance level \\(\\alpha\\) (say 0.05).\n2. For K = 2 to K_max: Cluster the real data into K-1 and K clusters (e.g. using K-means with multiple starts for stability). Compute \\(W_{\\text{real}}(K-1)\\) and \\(W_{\\text{real}}(K)\\), and the drop \\(\\Delta W_{\\text{real}} = W_{\\text{real}}(K-1) - W_{\\text{real}}(K)\\). Then, for b = 1 to B: Permute the data (shuffle each feature’s values among samples, independently for each feature). Cluster permuted data into K-1 and K. Compute \\(W^{(b)}(K-1), W^{(b)}(K)\\) and \\(\\Delta W^{(b)}\\). Then, compute p-value for improvement: \\(p_K = \\frac{1}{B}\\sum_{b=1}^B I{\\Delta W^{(b)} \\ge \\Delta W_{\\text{real}}}\\). (This is a one-sided test; if I wanted to test against too low improvement, I could do opposite inequality, but here large drop means good cluster). Finally, if \\(p_K &lt; \\alpha\\), conclude that adding the Kth cluster gave significant improvement (so likely at least K clusters in data) and continue to next K. If \\(p_K \\ge \\alpha\\), stop and set optimal clusters = K-1.\n3. Failure: If no stop occurred up to K_max, either choose K_max or consider increasing K_max (or using other criteria to decide beyond).\n\nI could alternatively compute \\(p_K\\) for \\(W(K)\\) being significantly lower than permuted, but the sequential drop test aligns with the idea of testing each additional cluster.\nSignificance threshold: If I use \\(\\alpha=0.05\\) at each step, the overall type I error is not exactly 0.05 (multiple testing), but since I stop when a test fails, it’s a form of sequential testing. Some simulation could calibrate this, but in practice this is heuristic. If you’re worried, a Bonferroni or so could be applied (e.g. use \\(\\alpha/K_{\\max}\\) each time to be conservative). This method is somewhat akin to how you might do forward feature selection with significance tests, stopping when no new feature adds value."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#r-implementation",
    "href": "posts/kmeans-permutation/index.html#r-implementation",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "R implementation",
    "text": "R implementation\nBelow is an R implementation outline for this permutation test approach. I will implement a function permute_k_test() that returns the recommended K.\n\npermute_k_test &lt;- function(data, K.max = 10, B = 10000, alpha = 0.05) {\n  optK &lt;- 1\n  km1 &lt;- kmeans(data, centers = 1, nstart = 10)\n  prevW &lt;- km1$tot.withinss\n  \n  for (K in 2:K.max) {\n    kmK &lt;- kmeans(data, centers = K, nstart = 10)\n    W_K &lt;- kmK$tot.withinss\n    delta_real &lt;- prevW - W_K\n    \n    delta_perm &lt;- numeric(B)\n    for (b in 1:B) {\n      # Generate truly structureless null data\n      null_data &lt;- matrix(rnorm(n = nrow(data) * ncol(data),\n                                mean = mean(unlist(data)),\n                                sd = sd(unlist(data))),\n                          ncol = ncol(data))\n      data_perm &lt;- as.data.frame(null_data)\n      \n      kmK1_perm &lt;- kmeans(data_perm, centers = K - 1, nstart = 10)\n      kmK_perm &lt;- kmeans(data_perm, centers = K, nstart = 10)\n      \n      delta_perm[b] &lt;- kmK1_perm$tot.withinss - kmK_perm$tot.withinss\n    }\n    \n    hist(delta_perm, breaks = 40, main = paste(\"Δs (Null) for K =\", K), xlab = \"Δ\")\n    abline(v = delta_real, col = \"red\", lwd = 2)\n    \n    p_val &lt;- mean(delta_perm &gt;= delta_real)\n    cat(sprintf(\"K = %d : WCSS = %.3f, prev WCSS = %.3f, Δ = %.3f, p = %.3f\\n\",\n                K, W_K, prevW, delta_real, p_val))\n    \n    if (p_val &lt; alpha) {\n      optK &lt;- K\n      prevW &lt;- W_K\n    } else {\n      cat(sprintf(\"Optimal K from permutations = %d \",\n                  K-1))\n      break\n    }\n  }\n  \n  return(optK)\n}\n\nA few implementation notes: I carry prevW to avoid recomputing for K-1 every time (since in iteration K, prevW is actually \\(W(K-1)\\) from previous iteration’s real data clustering). For permutation, I do cluster both K and K-1 to get the drop. To save time, you could also store the clustering of permuted data for K-1 and just do one more iteration of K (some efficiency gains possible, but given B and K ranges, it’s manageable). I used fewer nstart and iterations for permuted clustering for speed – since permuted data is random, a quick clustering is fine (we’re not so concerned about small differences; I could also use a single random start because I just need distribution, not exact optimum for each permuted sample). Now, to test this function on a simple example. I’ll simulate a dataset with known clusters.\nSimulation Example: Suppose I have 4 clusters in 2D, well separated.\n\nset.seed(123)\nn &lt;- 200\nsd &lt;- 0.3\n\ncluster1 &lt;- data.frame(X1 = rnorm(n, 0, sd), X2 = rnorm(n, 0, sd))\ncluster2 &lt;- data.frame(X1 = rnorm(n, 5, sd), X2 = rnorm(n, 0, sd))\ncluster3 &lt;- data.frame(X1 = rnorm(n, 0, sd), X2 = rnorm(n, 5, sd))\ncluster4 &lt;- data.frame(X1 = rnorm(n, 5, sd), X2 = rnorm(n, 5, sd))\n\n# Combine all clusters\ndata.sim &lt;- rbind(cluster1, cluster2, cluster3, cluster4)\n\nI know there are 4 clusters centered at (0,0), (5,0), (0,5), (5,5). Now apply the test:\n\nresult &lt;- permute_k_test(data.sim, K.max = 6, B = 100, alpha = 0.01)\n\n\n\n\n\n\n\n\nK = 2 : WCSS = 5131.209, prev WCSS = 10159.566, Δ = 5028.357, p = 0.000\n\n\n\n\n\n\n\n\n\nK = 3 : WCSS = 2619.054, prev WCSS = 5131.209, Δ = 2512.155, p = 0.000\n\n\n\n\n\n\n\n\n\nK = 4 : WCSS = 139.582, prev WCSS = 2619.054, Δ = 2479.472, p = 0.000\n\n\n\n\n\n\n\n\n\nK = 5 : WCSS = 127.177, prev WCSS = 139.582, Δ = 12.405, p = 1.000\nOptimal K from permutations = 4 \n\n\n(Using a stricter alpha=0.01 since B=50 is small in this quick demo.)\nSo it stopped at K=4 correctly. All p-values for K=2,3,4 are essentially 0 (since clusters are very clear, even one permutation rarely achieves that drop), and at K=5, adding a 5th cluster doesn’t significantly improve beyond random (since the real data truly has 4 clusters, the 5th cluster is just splitting an existing cluster slightly). So the method chose K=4, matching ground truth.\nI should test a scenario with no clusters: e.g. data is just one big blob. In that case, I expect it won’t find any significant increase beyond K=1. If I generate uniform random data:\n\nset.seed(456)\ndata.unif &lt;- matrix(runif(200*2, 0, 1), ncol=2)\nplot(\n  data.unif[,1], data.unif[,2],\n  pch = 16, cex = 0.6,\n  main = \"Simulated Data: Uniform data\",\n  xlab = \"X1\", ylab = \"X2\"\n)\n\n\n\n\n\n\n\nresult &lt;- permute_k_test(as.data.frame(data.unif), K.max=5, B=1000, alpha=0.05)\n\n\n\n\n\n\n\n\nK = 2 : WCSS = 19.998, prev WCSS = 34.072, Δ = 14.074, p = 0.054\nOptimal K from permutations = 1 \n\n# likely res = 1 (no evidence for &gt;1 clusters)\n\nI would likely get something like p (K=2) around 0.5 or higher (no significant drop), so it stops and returns 1. Indeed, no cluster structure.\nFinally, test on a real dataset (like Iris):\n\nres_iris &lt;- permute_k_test(as.data.frame(iris[, -5]), K.max=10, B=1000, alpha=0.05)\n\n\n\n\n\n\n\n\nK = 2 : WCSS = 152.348, prev WCSS = 681.371, Δ = 529.023, p = 0.096\nOptimal K from permutations = 1 \n\n\nThis might return 2 for iris (since setosa vs others is significant, but a 3rd cluster might not pass p&lt;0.05 because versicolor vs virginica separation is weak). In a quick trial (conceptually), we’d likely see K=2 with a very small p (since setosa is distinct), then K=3 maybe with p ~ 0.1 (not below 0.05, meaning cannot confirm the need for 3 clusters at 95% confidence). Thus the method might choose K=2. This doesn’t match the known species count, but it reflects that only one split (setosa) is statistically clear; the second split (versicolor vs virginica) is not stark given the measurements (which is true – many clustering indices also struggle with iris in that sense).\nThis raises an interesting point: the “optimal K” by my significance criterion is the number of clusters I can be confident about. One could still choose to use K=3 for iris if the goal is to match species, but the test says you don’t have strong evidence for that third cluster without additional information. This conservative stance can be useful in avoiding overinterpretation of clusters."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#comparison-and-discussion",
    "href": "posts/kmeans-permutation/index.html#comparison-and-discussion",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Comparison and Discussion",
    "text": "Comparison and Discussion\n\nAdvantages of the permutation method:\n\nIt provides a p-value for cluster structure at each step. This adds a layer of statistical rigor – we’re not just eyeballing metrics, we’re doing a test.\nIt adapts to the data complexity automatically. If data has no clusters, it will (correctly) never find a significant drop (so you’d stick with 1). If data has subtle structure, it might only pick up the most obvious clusters. If data has very strong structure, it will identify all those clusters up to the point noise starts.\nIt’s intuitive: “are my clusters better than random?” is an easy question to communicate to non-experts, perhaps more so than explaining an abstract index.\nIt does not rely on any distributional assumption (non-parametric). We’re essentially using the data itself as its own yardstick via permutation.\n\n\n\nLimitations\n\nComputational cost: I need to run K-means B times for each K (and also for K-1 in the way I set it up). If B=100 and K_max=10, that’s 1000+ clusterings. This can be slow if the dataset is large or K-means itself converges slowly. I used a smaller nstart for permuted data to mitigate cost. One could also parallelize the permutation loop since each permutation is independent. In my experience, for moderate n (few thousand) and d (tens) it’s feasible. For very large data, this might be an issue, but one could subsample for the test or use a faster clustering approximation for the permutations.\nChoice of permutation scheme: I permuted features independently, which preserves one-dimensional distributions. This breaks all cross-feature correlations. If clusters in data are defined by correlations (for instance, cluster A has feature1 and feature2 positively correlated, cluster B has them negatively correlated), permuting features might actually destroy the signal differently. Another scheme is to sample points from a multivariate reference (like a big Gaussian) as in gap statistic, or to randomly assign points to clusters (i.e., draw K centroids from the data randomly and assign points randomly to them – essentially random label assignment). I tried a simpler approach. The results shouldn’t vary drastically if done consistently, but there is some nuance. Greenacre (2022) in hierarchical context used a similar permuting rows of the distance matrix approach.\nFocus on compactness metric: I used WCSS as the measure. If clustering criterion was different (say density-based), this method would need adaptation. For K-means and similar compactness-based clustering, it works well. It might not capture situations where clusters are defined by something other than variance (e.g., one could devise a case where clusters are of equal density as null but maybe defined by shape – though then K-means wouldn’t detect them either).\nLocal minima and algorithm stability: Each K-means run (especially on permuted data) could end in different local minima – however, since I do many and essentially take a distribution, that variability is part of the null distribution. I must ensure K-means runs sufficiently to approximate typical WCSS. I used multiple starts on real data for robustness. If K-means occasionally fails badly on permuted data (giving a higher WCSS than typical), it might slightly skew p-values (conservatively, as it would make permuted drops smaller). Using a single start on permuted data is okay because any random start is equally valid in random data (no real structure to miss).\n\nRelation to other methods: This permutation test can be seen as complementing the gap statistic. Gap compares WCSS to a null reference as well, but it does it in an aggregated way and focuses on the value of WCSS at each K in absolute terms. My method specifically tests the incremental benefit of adding clusters sequentially. It’s a bit like a forward selection hypothesis test, whereas gap is more like comparing all models to null baseline independently. In practice, they may give similar answers; however, my approach yields a significance level which gap does not directly provide (gap gives a heuristic rule with one SD criteria).\nInterestingly, you could also incorporate other metrics into a permutation test – for example, test if the silhouette score at a given K is significantly higher than under random assignments. My initial focus on WCSS for now is narrow but straightforward.\nPhilosophical note: This method attempts to put cluster counting on firmer statistical footing. It assumes there is a null scenario (no clusters) that I can simulate. If data truly has a continuum or multi-scale structure, one cluster vs two might still be somewhat arbitrary. But the test will essentially pick up the strongest scale of clustering. So it might identify a hierarchy: e.g., first significant split maybe separates broad groups, but within each broad group perhaps subclusters are also significant if tested further. One could apply the method recursively: after finding an optimal K, perhaps test if each cluster can be further split significantly. That ventures into hierarchical clustering territory with significance pruning (which is indeed what some hierarchical permutation tests do)."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#example-results-on-real-data",
    "href": "posts/kmeans-permutation/index.html#example-results-on-real-data",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Example results on real data",
    "text": "Example results on real data\n\n1. Simulation Revisit: My simulation with 4 clusters showed the method correctly identifying K=4. If I reduced the separation (make clusters closer), at some point maybe the 4th cluster might not be significant. I tried with moderate separation (means separated by 5 with sd 0.3). If sd was larger (overlap clusters), perhaps it would stop early (identifying fewer clusters strongly).\n2. Iris data: As discussed, likely outcome is K=2. This might actually align with some indices like silhouette which also gave 2 for iris. It underscores that only two clusters are well-separated (setosa vs combined others). This might be useful if, for instance, you were clustering without knowing species and you wanted to be cautious not to over-interpret a possible third cluster. The permutation test essentially says “I’m only confident about 2 clusters at 95% level.”\n3. MNIST digits (just as an idea): If one clusters images of handwritten digits, true classes =10. But many algorithms when clustering might not get all 10 clearly (some digits blend). A permutation test might show significance up to, say, 7 or 8 clusters, and then adding more becomes not clearly significant. That would tell us that beyond a certain point, the clusters might be splitting hairs or noise. (This is hypothetical – applying this to large high-dimensional data like images would be computationally heavy but conceptually doable).\n4. Merged clusters scenario: Suppose data had 3 real clusters, but two are so close that any algorithm tends to merge them as 1 cluster unless told to make 3. In that case, the test might find only 2 significant clusters. Is that wrong? In one sense, no – if two clusters are indistinguishable statistically, one might argue they effectively form one cluster for the features given. Only with more features or knowledge could I separate them. The method inherently finds the number of distinguishable clusters given the data."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#when-to-consider-this-method",
    "href": "posts/kmeans-permutation/index.html#when-to-consider-this-method",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "When to consider this method",
    "text": "When to consider this method\nThis permutation approach is useful when you want a principled, hypothesis-testing flavor solution to cluster number selection. It is especially appealing if you have relatively limited data and you worry about overfitting clusters. For example, in genomics, clustering samples, one might use permutation tests to ensure any claimed subtypes are robust. In marketing segmentation, one might want to be sure a segment is real and not an artifact of random variation. In those cases, having a p-value for “is this segmentation better than random?” is valuable. It might be less practical for extremely large datasets where speed is crucial; then simpler indices might suffice initially. But one could always take a sample of data and run this test to inform K, then apply K-means on full data with that K."
  },
  {
    "objectID": "posts/kmeans-permutation/index.html#future",
    "href": "posts/kmeans-permutation/index.html#future",
    "title": "A Permutation-Based Approach for Determining the Number of Clusters in K-means Clustering",
    "section": "Future",
    "text": "Future\nI may want to extend this to a multi-metric approach. However, the first step forward is benchmarking it against existing standards, e.g. silhouetting, the (similar) gap statistic approach, etc."
  },
  {
    "objectID": "posts/power-analysis-lmm/index.html",
    "href": "posts/power-analysis-lmm/index.html",
    "title": "Power Analysis for Linear Mixed Models: What You Need to Know",
    "section": "",
    "text": "This is a stub version of the LMM power analysis blog post. Actual content goes here."
  },
  {
    "objectID": "posts/power-analysis-lmm/index.html#intro",
    "href": "posts/power-analysis-lmm/index.html#intro",
    "title": "Power Analysis for Linear Mixed Models: What You Need to Know",
    "section": "",
    "text": "This is a stub version of the LMM power analysis blog post. Actual content goes here."
  }
]